{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 다이나믹 프로그래밍\n",
        "다음 환경(**GridWorldEnvironment**)을 가지고 가치 이터레이션, 정책 이터레이션을 구현하세요."
      ],
      "metadata": {
        "id": "HyNmkw66VPdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GridWorldEnvironment\n",
        "```py\n",
        "start_point = (0,0)\n",
        "end_point = (4,4)\n",
        "grid_world_size = (5,5)\n",
        "env = GridWorldEnvironment(start_point, end_point, grid_world_size)\n",
        "```"
      ],
      "metadata": {
        "id": "AJazHY8WUeo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Original Code"
      ],
      "metadata": {
        "id": "FrEXfXlQW5CY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from typing import Tuple\n",
        "\n",
        "class GridWorldEnvironment:\n",
        "    def __init__(self, start_point:Tuple, end_point:Tuple, grid_world_size:Tuple):\n",
        "        # 시작점과 끝점을 받는다.\n",
        "        self.start_point = start_point\n",
        "        self.end_point = end_point if end_point != (-1,-1) else (grid_world_size[0] + end_point[0],\n",
        "                                                                 grid_world_size[1] + end_point[1])\n",
        "\n",
        "        # 그리드 월드의 규격을 받는다.\n",
        "        self.width, self.height = grid_world_size\n",
        "\n",
        "        # action dictionary\n",
        "        self.action_space = ['up', 'down', 'left', 'right']\n",
        "        self.num_actions = len(self.action_space)\n",
        "        self.actions = {'up':(-1,0),\n",
        "                        'down':(1,0),\n",
        "                        'left':(0,-1),\n",
        "                        'right':(0,1) }\n",
        "\n",
        "        # 상태 : 좌표로 나타남\n",
        "        self.traces = []\n",
        "\n",
        "        # total states\n",
        "        self.total_states = []\n",
        "        for x in range(self.width):\n",
        "            for y in range(self.height):\n",
        "                self.total_states.append((x,y))\n",
        "\n",
        "        # reward\n",
        "        self.reward = np.zeros(shape=(self.height, self.width)).tolist()\n",
        "        self.reward[end_point[0]][end_point[1]] = 1\n",
        "\n",
        "    def render(self):\n",
        "        # 그리드 월드의 상태를 출력한다.\n",
        "        self.grid_world = np.full(shape=(self.height, self.width), fill_value=\".\").tolist()\n",
        "\n",
        "        last_point = self.traces[-1] # 에이전트가 가장 마지막에 있었던 위치\n",
        "        traces = list(set(self.traces)) # 중복된 값을 삭제하기 위함\n",
        "        for trace in traces:\n",
        "            self.grid_world[trace[0]][trace[1]] = \"X\" #에이전트가 지나온 경로 표시\n",
        "\n",
        "        self.grid_world[self.start_point[0]][self.start_point[1]] = \"S\" # start point\n",
        "        self.grid_world[self.end_point[0]][self.end_point[1]] = \"G\" # end point\n",
        "        self.grid_world[last_point[0]][last_point[1]] = \"A\" # 현재 에이전트의 위치\n",
        "\n",
        "        # string으로 출력한다.\n",
        "        grid = \"\"\n",
        "\n",
        "        for i in range(self.height):\n",
        "            for j in range(self.width):\n",
        "                grid += self.grid_world[i][j]+\" \"\n",
        "            grid += \"\\n\"\n",
        "\n",
        "        print(grid)\n",
        "\n",
        "    def get_reward(self, state, action_idx):\n",
        "        next_state = self.state_after_action(state, action_idx)\n",
        "        return self.reward[next_state[0]][next_state[1]]\n",
        "\n",
        "    def state_after_action(self, state, action_idx:int):\n",
        "        action = self.action_space[action_idx]\n",
        "        row_movement, col_movement = self.actions[action]\n",
        "\n",
        "        # action에 따라 에이전트 이동\n",
        "        next_state = (state[0]+row_movement, state[1]+col_movement)\n",
        "        next_state = self.check_boundary(next_state)\n",
        "\n",
        "        return next_state\n",
        "\n",
        "    def check_boundary(self, state):\n",
        "        state = list(state)\n",
        "        state[0] = (0 if state[0] < 0 else self.height - 1 if state[0] > self.height - 1 else state[0])\n",
        "        state[1] = (0 if state[1] < 0 else self.width - 1 if state[1] > self.width - 1 else state[1])\n",
        "        return tuple(state)"
      ],
      "metadata": {
        "id": "m796J1R53VdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Own Code\n",
        "위 환경 클래스를 상속 받아 `env.render` 코드를 구현하세요."
      ],
      "metadata": {
        "id": "DHhCH3oBW79l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GridWorldEnvironment 클래스를 상속받아 render() 메서드를 호출하여 그리드월드를 출력하는 코드 구현!"
      ],
      "metadata": {
        "id": "N18LrNis7N6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RenderGridWorld(GridWorldEnvironment):\n",
        "    def __init__(self, start_point, end_point, grid_world_size):\n",
        "        super().__init__(start_point, end_point, grid_world_size)\n",
        "\n",
        "    def render(self):\n",
        "        super().render()\n",
        "\n",
        "# 환경 설정\n",
        "start_point = (0, 0)\n",
        "end_point = (4, 4)\n",
        "grid_world_size = (5, 5)\n",
        "\n",
        "# 환경 객체 생성\n",
        "env = RenderGridWorld(start_point, end_point, grid_world_size)\n",
        "\n",
        "# 그리드월드 출력\n",
        "env.traces = [(0, 0), (1, 0), (2, 0), (3, 0), (4, 0)]  # 에이전트의 이동 경로 추가\n",
        "env.render()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7XsRYRn3Kol",
        "outputId": "289a75ef-38e8-4e73-bbca-895b22366017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S . . . . \n",
            "X . . . . \n",
            "X . . . . \n",
            "X . . . . \n",
            "A . . . G \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 01. 정책 이터레이션\n",
        "- 스켈레톤 코드  \n",
        "`init` 부를 제외한 나머지 매소드를 채워주세요."
      ],
      "metadata": {
        "id": "LrGQnOj3VmdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyIteration:\n",
        "    def __init__(self,env):\n",
        "        # about env\n",
        "        self.env = env\n",
        "        self.action_space = env.action_space # ['up', 'down', 'left', 'right'] 차례대로 (-1,0) (1,0) (0,-1) (0,1)\n",
        "        self.num_action = len(self.action_space)\n",
        "\n",
        "        # value 2d list\n",
        "        self.value_table = np.zeros(shape = (env.height, env.width)).tolist() # Appendix 1\n",
        "\n",
        "        # 상하좌우 동일한 확률을 가지는 정책으로 초기화\n",
        "        self.policy_table = np.full(shape = (env.height, env.width, self.num_action), fill_value = 1/self.num_action).tolist()\n",
        "\n",
        "        # end state 에서는 어떠한 행동도 할 필요가 없어서 policy table을 비운다.\n",
        "        self.policy_table[env.end_point[0]][env.end_point[1]] = []\n",
        "\n",
        "        self.gamma = 0.9 # 할인율\n",
        "\n",
        "    # 상태에 따른 정책 반환\n",
        "    def get_policy(self, state):\n",
        "        return self.policy_table[state[0]][state[1]]\n",
        "\n",
        "    # 가치 함수의 값을 반환\n",
        "    def get_value(self, state):\n",
        "        return self.value_table[state[0]][state[1]]\n",
        "\n",
        "    # 특정 상태에서 정책에 따라 무작위로 행동을 반환\n",
        "    def get_action(self,state):\n",
        "        policy = self.get_policy(state)\n",
        "        policy = np.array(policy)\n",
        "        return np.random.choice(4, 1, p=policy)[0]\n",
        "\n",
        "    # 벨만 기대 방정식을 통해 다음 가치함수를 계산하는 정책 평가\n",
        "    def policy_evaluation(self):\n",
        "        # 다음 가치함수 초기화\n",
        "        next_value_table = [[0.00] * self.env.width\n",
        "                           for _ in range(self.env.height)]\n",
        "\n",
        "        # 모든 상태에 대해서 벨만 기대방정식을 계산\n",
        "        for state in self.env.get_all_states():\n",
        "            value = 0.0\n",
        "            # 마침 상태의 가치 함수 = 0\n",
        "            if state == [2, 2]:\n",
        "                next_value_table[state[0]][state[1]] = value\n",
        "                continue\n",
        "\n",
        "            # 벨만 기대 방정식\n",
        "            for action in self.env.possible_actions:\n",
        "                next_state = self.env.state_after_action(state, action)\n",
        "                reward = self.env.get_reward(state, action)\n",
        "                next_value = self.get_value(next_state)\n",
        "                value += (self.get_policy(state)[action] *\n",
        "                          (reward + self.discount_factor * next_value))\n",
        "\n",
        "            next_value_table[state[0]][state[1]] = value\n",
        "\n",
        "        self.value_table = next_value_table\n",
        "\n",
        "    # 현재 가치 함수에 대해서 탐욕 정책 발전\n",
        "    def policy_improvement(self):\n",
        "        next_policy = self.policy_table\n",
        "        for state in self.env.get_all_states():\n",
        "            if state == [2, 2]:\n",
        "                continue\n",
        "\n",
        "            value_list = []\n",
        "            # 반환할 정책 초기화\n",
        "            result = [0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "            # 모든 행동에 대해서 [보상 + (할인율 * 다음 상태 가치함수)] 계산\n",
        "            for index, action in enumerate(self.env.possible_actions):\n",
        "                next_state = self.env.state_after_action(state, action)\n",
        "                reward = self.env.get_reward(state, action)\n",
        "                next_value = self.get_value(next_state)\n",
        "                value = reward + self.discount_factor * next_value\n",
        "                value_list.append(value)\n",
        "\n",
        "            # 받을 보상이 최대인 행동들에 대해 탐욕 정책 발전\n",
        "            max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
        "            max_idx_list = max_idx_list.flatten().tolist()\n",
        "            prob = 1 / len(max_idx_list)\n",
        "\n",
        "            for idx in max_idx_list:\n",
        "                result[idx] = prob\n",
        "\n",
        "            next_policy[state[0]][state[1]] = result\n",
        "\n",
        "        self.policy_table = next_policy"
      ],
      "metadata": {
        "id": "3cSmPtYxVqrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 정책 이터레이션 실행\n",
        "정의한 클래스를 이용해 정책 이터레이션을 실행해 주세요."
      ],
      "metadata": {
        "id": "vHhUyl4PYSez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 정책 이터레이션 함수 추가\n",
        "def policy_iteration(self, max_iterations=100):\n",
        "    for _ in range(max_iterations):\n",
        "        old_policy = np.copy(self.policy_table)\n",
        "        self.policy_evaluation()\n",
        "        self.policy_improvement()\n",
        "\n",
        "        if np.all(np.array(old_policy) == np.array(self.policy_table)):\n",
        "           print(\"정책이 수렴되었습니다.\")\n",
        "           break\n",
        "# 최종 정책 출력 함수\n",
        "def print_policy(policy_table, env):\n",
        "    for i in range(env.height):\n",
        "        for j in range(env.width):\n",
        "            if (i, j) == env.end_point:\n",
        "                print(\"G\", end=\"\\t\")  # 도착점\n",
        "            else:\n",
        "                print(np.argmax(policy_table[i][j]), end=\"\\t\")  # 가장 높은 확률의 행동 출력\n",
        "        print(\"\\n\")\n",
        "# 정책 이터레이션 실행\n",
        "policy_iteration = PolicyIteration(env)\n",
        "\n",
        "# 최종 정책 출력\n",
        "print_policy(policy_iteration, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "o7dLYlXT-IDm",
        "outputId": "83aaffb7-7ad3-4113-e32c-abdb776b7591"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'PolicyIteration' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-fcdf3c738ef7>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# 최종 정책 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mprint_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-fcdf3c738ef7>\u001b[0m in \u001b[0;36mprint_policy\u001b[0;34m(policy_table, env)\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"G\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 도착점\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 가장 높은 확률의 행동 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# 정책 이터레이션 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'PolicyIteration' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습에서 중요한 정보\n",
        "학습이 되고 있는 것을 판단할 수 있는 파라미터를 찾아 시각화해 주세요."
      ],
      "metadata": {
        "id": "j2FMBu8zX7Kg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sr3VchPRYpBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dSlI8R_KY2-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02. 가치 이터레이션\n",
        "- 스켈레톤 코드  \n",
        "`init` 부를 제외한 나머지 매소드를 채워주세요."
      ],
      "metadata": {
        "id": "8tutibdyYwAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueIteration:\n",
        "    def __init__(self, env):\n",
        "\n",
        "        self.env = env\n",
        "        self.action_space = env.action_space\n",
        "        self.num_actions = env.num_actions\n",
        "\n",
        "        # value 2d list\n",
        "        self.value_table = np.zeros(shape=(env.height, env.width)).tolist()\n",
        "\n",
        "        self.gamma = 0.9 # 할인율\n",
        "\n",
        "    # 벨만 최적 방정식을 통해 다음 가치 함수 계산\n",
        "    def value_iteration(self):\n",
        "        # 다음 가치함수 초기화\n",
        "        next_value_table = [[0.0] * self.env.width\n",
        "                           for _ in range(self.env.height)]\n",
        "\n",
        "        # 모든 상태에 대해서 벨만 최적방정식을 계산\n",
        "        for state in self.env.get_all_states():\n",
        "            # 마침 상태의 가치 함수 = 0\n",
        "            if state == [2, 2]:\n",
        "                next_value_table[state[0]][state[1]] = 0.0\n",
        "                continue\n",
        "\n",
        "            # 벨만 최적 방정식\n",
        "            value_list = []\n",
        "            for action in self.env.possible_actions:\n",
        "                next_state = self.env.state_after_action(state, action)\n",
        "                reward = self.env.get_reward(state, action)\n",
        "                next_value = self.get_value(next_state)\n",
        "                value_list.append((reward + self.discount_factor * next_value))\n",
        "\n",
        "            # 최댓값을 다음 가치 함수로 대입\n",
        "            next_value_table[state[0]][state[1]] = max(value_list)\n",
        "\n",
        "        self.value_table = next_value_table\n",
        "\n",
        "    # 현재 가치 함수로부터 행동을 반환\n",
        "    def get_action(self, state):\n",
        "        if state == [2, 2]:\n",
        "            return []\n",
        "\n",
        "        # 모든 행동에 대해 큐함수 (보상 + (감가율 * 다음 상태 가치함수))를 계산\n",
        "        value_list = []\n",
        "        for action in self.env.possible_actions:\n",
        "            next_state = self.env.state_after_action(state, action)\n",
        "            reward = self.env.get_reward(state, action)\n",
        "            next_value = self.get_value(next_state)\n",
        "            value = (reward + self.discount_factor * next_value)\n",
        "            value_list.append(value)\n",
        "\n",
        "        # 최대 큐 함수를 가진 행동(복수일 경우 여러 개)을 반환\n",
        "        max_idx_list = np.argwhere(value_list == np.amax(value_list))\n",
        "        action_list = max_idx_list.flatten().tolist()\n",
        "        return action_list\n",
        "\n",
        "    def get_value(self, state):\n",
        "        return self.value_table[state[0]][state[1]]"
      ],
      "metadata": {
        "id": "-rnooRxnYwAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 가치 이터레이션 실행\n",
        "정의한 클래스를 이용해 가치 이터레이션을 실행해 주세요."
      ],
      "metadata": {
        "id": "d073zNNfYwAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 정책 출력 함수\n",
        "def print_policy(value_iteration, env):\n",
        "    for i in range(env.height):\n",
        "        for j in range(env.width):\n",
        "            if (i, j) == env.end_point:\n",
        "                print(\"G\", end=\"\\t\")\n",
        "            else:\n",
        "                actions = value_iteration.get_action([i, j])\n",
        "                print(actions, end=\"\\t\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "# 가치 이터레이션 실행\n",
        "value_iteration = ValueIteration(env)\n",
        "\n",
        "# 최종 정책 출력\n",
        "print_policy(value_iteration, env)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "bFOumYYE-Odt",
        "outputId": "bd89d191-5e0d-44ac-f185-2e9867ec3d5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'RenderGridWorld' object has no attribute 'possible_actions'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-05c77c20e0c3>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# 최종 정책 출력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-34-05c77c20e0c3>\u001b[0m in \u001b[0;36mprint_policy\u001b[0;34m(value_iteration, env)\u001b[0m\n\u001b[1;32m      6\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"G\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                 \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-9a3745df1dcf>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;31m# 모든 행동에 대해 큐함수 (보상 + (감가율 * 다음 상태 가치함수))를 계산\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mvalue_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpossible_actions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_after_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'RenderGridWorld' object has no attribute 'possible_actions'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습에서 중요한 정보\n",
        "학습이 되고 있는 것을 판단할 수 있는 파라미터/요소를 찾아 시각화해 주세요."
      ],
      "metadata": {
        "id": "sR6OuQVAYwAW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aWa6ZpsCYwAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MT9b89DiZFZG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}